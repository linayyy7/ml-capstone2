# ğŸš€ **Remote Worker Productivity Prediction Service**

This repository contains a complete machine learning project that predicts the task completion rate of remote workers ğŸ“ˆ based on their work behavior, experience, and scheduling patterns. The project demonstrates an end-to-end machine learning workflow, including data analysis, model training, evaluation, and deployment as a REST API using Docker ğŸ³. This project is developed as part of the ML Zoomcamp program ğŸ“.

## ğŸŒ **Background and Motivation**

With the increasing adoption of remote and hybrid work models, organizations face challenges in understanding employee productivity without direct supervision ğŸ‘€. Traditional productivity metrics are often subjective or difficult to measure consistently.

This project investigates whether observable behavioral signalsâ€”such as daily working hours, break frequency, experience level, and calendar usageâ€”can be used to quantitatively predict task completion performance for remote workers using supervised machine learning ğŸ¤–.

## ğŸ“Š **Dataset Description**

Each record in the dataset represents one remote worker along with productivity-related metrics.

The input features used in this project include:

**location_type** â€” worker's location category (e.g., Urban, Suburban, Rural)  
**industry_sector** â€” industry of employment  
**age** â€” worker age  
**experience_years** â€” total years of professional experience  
**average_daily_work_hours** â€” average number of hours worked per day  
**break_frequency_per_day** â€” number of breaks taken per day  
**calendar_scheduled_usage** â€” fraction of work time scheduled via a calendar  
**late_task_ratio** â€” proportion of tasks completed after their deadline  

The target variable is **task_completion_rate**, which represents the proportion of assigned tasks completed successfully.

The column **worker_id** is used only as an identifier and is removed during preprocessing.

The dataset file should be placed at:  
`data/remote_worker_productivity.csv`

## ğŸ—‚ï¸ **Project Structure**

The repository is organized as follows:
## Project Structure

```text
â”œâ”€â”€ README.md                  # Project documentation (this file)
â”œâ”€â”€ notebook.ipynb             # Exploratory data analysis
â”œâ”€â”€ train.py                   # Model training script
â”œâ”€â”€ predict.py                 # Prediction API
â”œâ”€â”€ model.bin                  # Trained model artifact
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ Dockerfile                 # Container configuration
â””â”€â”€ data/
    â””â”€â”€ remote_worker_productivity.csv
```



## âš™ï¸ **Machine Learning Workflow**

The project follows a standard and reproducible machine learning workflow ğŸ”.

**1. Exploratory Data Analysis (EDA)**  
Performed to understand the distribution of the target variable, analyze relationships between features and productivity, and identify potential data quality issues such as outliers.

**2. Preprocessing and Feature Engineering**  
Categorical features are encoded using one-hot encoding, while numerical features are passed directly. Preprocessing and modeling are combined into a single pipeline to ensure consistency between training and inference.

**3. Modeling**  
A baseline linear regression model is trained for comparison, followed by a **Random Forest Regressor ğŸŒ²** as the final model. Model performance is evaluated using RMSE (Root Mean Squared Error), and the best-performing model is selected.

**4. Model Serialization**  
The trained pipelineâ€”including preprocessing stepsâ€”is serialized and saved as `model.bin` ğŸ’¾.

## ğŸ‹ï¸ **Model Training**

Model training is performed using the `train.py` script.  
The script loads the dataset, splits the data into training and test sets, trains the model, evaluates its performance on the test data, and saves the trained model artifact to disk.

## ğŸŒ **Prediction API**

A lightweight REST API is implemented using Flask ğŸ§ª to serve predictions.

When the service is started, it listens on port `9696` ğŸ”Œ and exposes two endpoints:

**GET /health** â€” returns a health check response  
**POST /predict** â€” returns a predicted task completion rate  

The prediction endpoint accepts worker attributes as JSON input and returns a single value: `predicted_task_completion_rate`.

## ğŸ³ **Docker Deployment**

The prediction service can be packaged and deployed locally using Docker ğŸ³.

A Docker image is built using the provided `Dockerfile`, and the container exposes port `9696`, allowing the API to be accessed at:  
`http://localhost:9696`

## ğŸ“¦ **Dependencies**

All required Python dependencies are listed in `requirements.txt`.  
The main libraries used in this project include `pandas`, `numpy`, `scikit-learn`, `Flask`, and `gunicorn`.

## âœ… **ML Zoomcamp Deliverables**

This project fulfills all ML Zoomcamp requirements, including:

- Clear problem definition
- Dataset documentation and usage instructions
- Exploratory data analysis
- Feature engineering and model selection
- Reproducible training scripts
- Model serialization
- Prediction web service
- Dockerized deployment

## ğŸ”® **Limitations and Future Work**

Future improvements may include:

- Experimenting with gradient boosting models
- Adding model explainability techniques such as SHAP ğŸ“Š
- Performing subgroup error analysis
- Deploying the service to a cloud platform â˜ï¸

